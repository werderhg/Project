{"name":"Project","tagline":"August Homework","body":"---\r\ntitle: \"Project Machine Learning Coursera Hopekins\"\r\nauthor: \"Harry Werder\"\r\ndate: \"Thursday, July 09, 2015\"\r\noutput: html_document\r\n---\r\n\r\n### Introduction\r\nWe will build a model to predict the outcome \"classe\" with the levels A,B,C,D,E based on covariants (predictors) from the Human Activity Recognition data.\r\nThe data for this project comes from this source: http://groupware.les.inf.puc-rio.br/har. \r\n\r\n###1. Step: Set-up of environment in R\r\n\r\n```{r}\r\nsetwd(\"H:/Eigene Dateien/A-Coursera/Stat Kurs/8-Machine Learning/Project\") #working directory\r\nrm(list=ls()) #clean the variables\r\nlibrary(ggplot2); library(caret); library(randomForest) #packages used\r\n\r\n#Use multiple cores of the processor\r\n# library(doParallel) #install.packages(\"doParallel\")\r\n# registerDoParallel(cores=2)\r\n```\r\n\r\n\r\n### 2. Step: Loading the data\r\nWe load the data and replace all \"\",\"NA\", \"#DIV/0!\" values with the NA value from R.\r\n```{r}\r\nrawdata <- read.csv(\"pml-training.csv\", na.strings = c(\"\", \"NA\", \"#DIV/0!\") )\r\n```\r\n\r\n### 3. Step: Clean the data and select covariant\r\nWe found out by reading the data description that the first seven columns are descriptions.\r\nSo we remove them from our raw data data. -(1:7).\r\n```{r}\r\ndata <- rawdata[,-(1:7)]\r\n```\r\n\r\nWe remove now the covariants that have more than 10% of missing data.\r\n\r\n```{r}\r\n# set the outcome aside\r\nclasse <- data$classe\r\n    # remove the classe from the data\r\n        #find the column with the covariants (predictors)\r\n        aa<- colnames(data)==\"classe\"\r\n\r\n    data <- data[,!aa]\r\n# define a function for the removal\r\nremove_cov <- function(x,y) { \r\n    shortdata <- x[ , colSums( is.na(x) ) <= y/100*nrow(x) ]\r\n    bb <- c(i, ncol(shortdata))\r\n    return(bb)\r\n    }\r\nbbb<- data.frame(percent=integer(),covariant=integer()) #initiate\r\n# i is the percentage of NA's per column\r\nfor(i in 0:100) {\r\n    bbb[i+1,1]<-remove_cov(data,i)[1]\r\n    bbb[i+1,2]<-remove_cov(data,i)[2]\r\n    }\r\n#the plot tells us what percentage cut-off will reduce covariants\r\nqplot(percent,covariant,data=bbb)\r\n\r\n# create our reduced data\r\nshortdata <- data[ , colSums( is.na(data) ) <= 10/100*nrow(data) ]\r\n\r\n#reduce by PCA\r\ncc<-data.frame(percent=integer(),components=integer())\r\nfor( i in 1:99) {\r\n    pcaFit<-preProcess(shortdata, method=c(\"center\", \"scale\", \"pca\"),thresh=i/100)\r\n    cc[i,1]<- i\r\n    cc[i,2]<-pcaFit$numComp\r\n    }\r\nqplot(percent,components,data=cc)\r\n\r\npcaFit95<-preProcess(shortdata, method=c(\"center\", \"scale\", \"pca\"),thresh=0.95)\r\npcadata95<-predict(pcaFit95,shortdata)\r\npcaFit75<-preProcess(shortdata, method=c(\"center\", \"scale\", \"pca\"),thresh=0.75)\r\npcadata75<-predict(pcaFit75,shortdata)\r\n\r\n# add the outcome\r\nshortdata$classe <- classe\r\npcadata95$classe<-classe\r\npcadata75$classe<-classe\r\n```\r\n\r\nWe have reduced the number of covariants from **`r ncol(data) `** to **`r ncol(shortdata)-1 `**.\r\n\r\n```{r}\r\nyaa <- levels(shortdata$classe)\r\n```\r\nThe outcome has the following levels **`r yaa`**.\r\n\r\n\r\n### 4. Setting up our training and test datasets\r\nWe split the data into a training set (70%) and a test set (30%).\r\n\r\n```{r}\r\nset.seed(400)\r\ninTrain <-createDataPartition(y=shortdata$classe, p=0.7, list=FALSE)\r\ntraining <- shortdata[inTrain,]; \r\ntesting <- shortdata[-inTrain,]\r\n\r\ninTrain95 <-createDataPartition(y=pcadata95$classe, p=0.3, list=FALSE)\r\ntraining95 <- pcadata95[inTrain95,]; \r\ntesting95 <- pcadata95[-inTrain95,]\r\n\r\ninTrain75 <-createDataPartition(y=pcadata75$classe, p=0.3, list=FALSE)\r\ntraining75 <- pcadata75[inTrain75,]; \r\ntesting75 <- pcadata75[-inTrain75,]\r\n\r\n\r\n\r\n```\r\n\r\nWe have now a training data set with **`r dim(training) `** rows.\r\n\r\n### 5. Step: Calculate the model\r\n\r\n#### 5.1. Using k-nearest neighbors (knn)\r\nknn-methods is a very simple method. The main parameter is the number of neighbors (=k). \r\n\r\n```{r}\r\nset.seed(400)\r\nctrl <- trainControl(method=\"repeatedcv\",repeats = 3) #,classProbs=TRUE,summaryFunction = twoClassSummary)\r\n# knnFit <- train(classe ~ ., data = training, method = \"knn\", trControl = ctrl, preProcess = c(\"center\",\"scale\"), tuneLength = 20)\r\n\r\n# saveRDS(knnFit, file=\"knnFit7030.rds\") #save the result\r\nknnFit <- readRDS(\"knnFit7030.rds\")\r\n\r\n```\r\n\r\nThe following plots tell us more. (training data)\r\n```{r}\r\nknnplot<- plot(knnFit)\r\n```\r\n\r\nThe best accuracy is achieved with k=5. We note that the in-sample accuracy is above 95%.\r\n\r\n\r\n#### 5.2 Using random forest method\r\n\r\nWe chosen  random tree as the algorith because it is recognized as a good performer.\r\n```{r}\r\n# rfFit <- train( classe ~ ., method=\"rf\", prox=TRUE, data=training)\r\n# saveRDS(rfFit, file=\"rfFit7030.rds\")\r\nrfFit <-readRDS(\"rfFit7030.rds\")\r\ntt <- rfFit$finalModel\r\n```\r\nThe resulting model\r\n```{r}\r\ntt\r\n```\r\nOOB looks great with 1.75%!!\r\n\r\nWe also use next our pca-reduced dataset with less covariants with random tree.\r\nOne model with 75% variance explained by pca and the other with 95% variance explained.\r\n```{r}\r\n# it is very timeconsuming to do the train, so I show code and load from file the result\r\n# rfFit95 <- train( classe ~ ., method=\"rf\", prox=TRUE, data=training95)\r\n# saveRDS(rfFit95, file=\"rfFit953070.rds\")\r\nrfFit95 <-readRDS(\"rfFit953070.rds\")\r\ntt95 <- rfFit95$finalModel\r\n\r\n# rfFit75 <- train( classe ~ ., method=\"rf\", prox=TRUE, data=training75)\r\n# saveRDS(rfFit75, file=\"rfFit753070.rds\")\r\nrfFit75 <- readRDS(\"rfFit753070.rds\")\r\ntt75 <- rfFit75$finalModel\r\n```\r\n\r\nThe resulting model for 75% variance explained pca:\r\n```{r}\r\ntt75\r\n```\r\nThe resulting model for 75% variance explained pca:\r\n```{r}\r\ntt95\r\n```\r\n\r\nOOB estimate of error rate is for both model low but fare away from the non adjusted.\r\n\r\n\r\n### 5. Testing the model with the test data\r\n\r\nWe use now our testing data to see how good our model works.\r\n\r\nEvaluation for pca-reduced models\r\n```{r}\r\n\r\n#our model with PCA at 95% explained, training 30% and 70% test\r\ntestaa<- colnames(testing)==\"classe\"\r\ntest95<-testing[,!(testaa)]\r\ntestpcadata95<-predict(pcaFit95,test95)\r\ntestpcadata95$classe <-testing$classe\r\npred95<-predict(rfFit95,testpcadata95)\r\ntest95conf <- confusionMatrix(testing$classe,pred95)\r\naccrfpca95<-test95conf$overall[1]\r\n\r\n#our model with PCA at 75% explained, training 20% and 80% test\r\ntest75<-testing[,!(testaa)]\r\ntestpcadata75<-predict(pcaFit75,test75)\r\ntestpcadata75$classe <-testing$classe\r\npred75<-predict(rfFit75,testpcadata75)\r\ntest75conf <- confusionMatrix(testing$classe,pred75)\r\naccrfpca75<-test75conf$overall[1]\r\n```\r\n\r\nAccuracy is for the 75% variance explained pca: **`r accrfpca75 `**\r\nAccuracy is for the 95% variance explained pca: **`r accrfpca95 `**\r\n\r\nNow we test with the full random forest model:\r\n```{r}\r\ntest100<-testing[,!(testaa)]\r\n\r\npredrf<-predict(rfFit,test100)\r\ntestrfconf <- confusionMatrix(testing$classe,predrf)\r\naccrf<-testrfconf$overall[1]\r\n```\r\n\r\nAccuracy is for the full random forest model predication: **`r accrf `**\r\n\r\nNow we test with the k-nearest neighbors:\r\n```{r}\r\n# using the test data\r\n# predknn<- predict(knnFit,testing)\r\n# saveRDS(predknn, file=\"knnFitprdict.rds\")\r\npredknn <- readRDS(\"knnFitprdict.rds\")\r\nttknn<-confusionMatrix(testing$classe, predknn)\r\naccknn<-ttknn$overall[1]\r\n```\r\nAccuracy is for the knn predication: **`r accknn `**\r\n\r\nSummary\r\nWe recommend to use random forest model with accuracy of **`r accrf `**","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}